{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdU5xARDUuZT"
   },
   "source": [
    "# MLBio- HW05\n",
    "## Generative Adversarial Networks\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Name:\n",
    "\n",
    "Student No.:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsAn8FaVRrDR"
   },
   "source": [
    "**!!! CAUTION !!!**\n",
    "\n",
    "* To prevent Colab from disconnecting, insert the following javascript code into the inspector's console.\n",
    "\n",
    "\n",
    "More info: \n",
    "[medium](https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0),\n",
    "[stackoverflow](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting)\n",
    "```\n",
    "function ClickConnect(){\n",
    "  console.log(\"Working\"); \n",
    "  document\n",
    "    .querySelector(\"#top-toolbar > colab-connect-button\")\n",
    "    .shadowRoot\n",
    "    .querySelector(\"#connect\")\n",
    "    .click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhsBAXcA8fM3"
   },
   "outputs": [],
   "source": [
    "#################### Problem 00 ####################\n",
    "# Remember to write your Name and Student No. in the first cell :D\n",
    "####################### End ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXZkOH_3G5zY"
   },
   "outputs": [],
   "source": [
    "!pip install gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Yuka_u595a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EByRj-NrnNNl"
   },
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RG7eTDahnMHl"
   },
   "outputs": [],
   "source": [
    "# Reproducibility options\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if CUDA:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvGS0IrxoPJa"
   },
   "source": [
    "# 1) Auxiliary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTDUiGk8oZmY"
   },
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "  \"\"\"\n",
    "  This module reshapes its input to `new_shape`\n",
    "  \"\"\"\n",
    "  def __init__(self, new_shape):\n",
    "    super().__init__()\n",
    "    self.new_shape = new_shape\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x.view(-1, *self.new_shape)\n",
    "\n",
    "class NoiseGenerator(nn.Module):\n",
    "  \"\"\"\n",
    "  This module generates `n` noises with `z_dim` dim from Normal distribution.\n",
    "  \"\"\"\n",
    "  def __init__(self, z_dim):\n",
    "    super().__init__()\n",
    "    self.z_dim = z_dim\n",
    "\n",
    "  def forward(self, n):\n",
    "    noise = torch.randn(n, *self.z_dim).to(device)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaFukH-jmsPR"
   },
   "source": [
    "# 2) GAN Training Structure (14 points)\n",
    "\n",
    "Complete the following class to define the GAN training structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAttifcsc-6P"
   },
   "outputs": [],
   "source": [
    "class GANTrainer:\n",
    "  \"\"\"\n",
    "  This class wraps the GAN training structure.\n",
    "\n",
    "  Some arguments: \\n\n",
    "  `visualization_helper_fn`: Used to visualize model outputs at the end of each epoch. \\n\n",
    "  `G_loss_mode`: `{\"logD\" | \"log(1-D)\"}` Controls the loss function of the `generator`. \\n\n",
    "  `generator`: The generator. \\n\n",
    "  `discriminator`: The discriminator. The last layer is a LINEAR layer. \\n\n",
    "  \"\"\"\n",
    "  def __init__(self, train_dataloader:torch.utils.data.DataLoader, test_dataloader:torch.utils.data.DataLoader,\n",
    "               noise_generator:NoiseGenerator, generator:nn.Module, discriminator:nn.Module,\n",
    "               G_lr, D_lr,\n",
    "               visualization_helper_fn, G_loss_mode):\n",
    "    self.train_dataloader = train_dataloader\n",
    "    self.test_dataloader = test_dataloader\n",
    "\n",
    "    self.noise_generator = noise_generator.to(device)\n",
    "    self.generator = generator.to(device)\n",
    "    self.discriminator = discriminator.to(device)\n",
    "\n",
    "    self.visualization_helper_fn = visualization_helper_fn\n",
    "    self.G_loss_mode = G_loss_mode\n",
    "    \n",
    "    #################### Problem 01 (1 points) ####################\n",
    "    # Define Adam optimizers with `G_lr` and `D_lr` learning rates.\n",
    "    ####################################################\n",
    "    self.opt_G = pass\n",
    "    self.opt_D = pass\n",
    "    ####################### End ########################\n",
    "\n",
    "  def generate_samples(self, n):\n",
    "    \"\"\"\n",
    "    This function generates `n` samples.\n",
    "    \"\"\"\n",
    "    #################### Problem 02 (1 points) ####################\n",
    "    # 1) Generate `n` noises\n",
    "    # 1) Transform noises to samples\n",
    "    ####################################################\n",
    "    noise = pass\n",
    "    fake_samples = pass\n",
    "    ####################### End ########################\n",
    "    return noise, fake_samples\n",
    "\n",
    "  def G_step(self, n):\n",
    "    \"\"\"\n",
    "    This function calculates `generator`'s loss for `n` samples.\n",
    "    \"\"\"\n",
    "    #################### Problem 03 (1 points) ####################\n",
    "    # 1) Generate `n` fake samples. put them in `x_fake`\n",
    "    # 2) Compute discriminator `logits` for generated samples\n",
    "    ####################################################\n",
    "    x_fake = pass\n",
    "    logits = pass\n",
    "    ####################### End ########################\n",
    "\n",
    "    if self.G_loss_mode == 'logD':\n",
    "      #################### Problem 04 (2 points) ####################\n",
    "      # Compute the `generator`'s loss when using `logD` as an objective.\n",
    "      # `loss = mean(...)`\n",
    "      #\n",
    "      # CAUTION!!!!! --->>>> PAY ATTENTION TO THE SIGN OF LOSS\n",
    "      ####################################################\n",
    "      loss = pass\n",
    "      ####################### End ########################\n",
    "\n",
    "    elif self.G_loss_mode == 'log(1-D)':\n",
    "      #################### Problem 05 (2 points) ####################\n",
    "      # Compute the `generator`'s loss when using `log(1-D)` as an objective.\n",
    "      # `loss = mean(...)`\n",
    "      #\n",
    "      # CAUTION!!!!! --->>>> PAY ATTENTION TO THE SIGN OF LOSS\n",
    "      ####################################################\n",
    "      loss = pass\n",
    "      ####################### End ########################\n",
    "\n",
    "    else:\n",
    "      raise BaseException('Invalid generator train mode!!')\n",
    "      \n",
    "    return loss, x_fake\n",
    "\n",
    "  def D_step(self, x_real):\n",
    "    \"\"\"\n",
    "    This function calculates `discriminator`'s loss for `x_real` and `x_fake`.\n",
    "    \"\"\"\n",
    "    #################### Problem 06 (1 points) ####################\n",
    "    # 1) Generate `n` fake samples. What is the value of `n`? why?\n",
    "    #  put them in `x_fake`\n",
    "    # 2) Detach the generated samples. why?\n",
    "    ####################################################\n",
    "    x_fake = pass\n",
    "    ####################### End ########################\n",
    "\n",
    "    #################### Problem 07 (2 points) ####################\n",
    "    # Compute discriminator `loss` for `x_real` and `x_fake`. \n",
    "    # `loss = mean(...)`\n",
    "    ####################################################\n",
    "    loss = pass\n",
    "    ####################### End ########################\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def train_loop(self, epoch, G_update_times, D_update_times, verbose=True):\n",
    "    \"\"\"\n",
    "    This function iterates over `train_dataloader` and trains \n",
    "    `generator` and `discriminator` for ONE epoch.\n",
    "\n",
    "    Some arguments: \\n\n",
    "    `G_update_times`: how many times to update `generator` on each batch \\n\n",
    "    `D_update_times`: how many times to update `discriminator` on each batch \\n\n",
    "    \"\"\"\n",
    "    train_G_loss = 0\n",
    "    train_D_loss = 0\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(self.train_dataloader)):\n",
    "      x = x.to(device)\n",
    "\n",
    "      for _ in range(D_update_times):\n",
    "        #################### Problem 08 (1 points) ####################\n",
    "        # 1) Put `generator` in `eval` mode\n",
    "        # 2) Put `discriminator` in `train` mode\n",
    "        # 3) Zero out `opt_D`'s gradients\n",
    "        # 4) Compute `discriminator`'s loss (`D_loss`); use `D_step`\n",
    "        # 5) Backpropagate `D_loss`\n",
    "        # 6) Update ``discriminator` parameters\n",
    "        ####################################################\n",
    "        pass\n",
    "        D_loss = pass\n",
    "        pass\n",
    "        ####################### End ########################\n",
    "\n",
    "      for _ in range(G_update_times):\n",
    "        #################### Problem 09 (1 points) ####################\n",
    "        # 1) Put `generator` in `train` mode\n",
    "        # 2) Put `discriminator` in `eval` mode\n",
    "        # 3) Zero out `opt_G`'s gradients\n",
    "        # 4) Compute `generator`'s loss (`G_loss`); use `G_step`\n",
    "        # 5) Backpropagate `G_loss`\n",
    "        # 6) Update `generator` parameters\n",
    "        ####################################################\n",
    "        pass\n",
    "        G_loss = pass\n",
    "        pass\n",
    "        ####################### End ########################\n",
    "      \n",
    "      train_D_loss += D_loss.item() * x.shape[0]\n",
    "      train_G_loss += G_loss.item() * x.shape[0]\n",
    "\n",
    "      if verbose and batch_idx % LOG_INTERVAL == 0:\n",
    "          print('Train | Epoch: {} [{}/{}]\\t\\tD-Loss: {:.6f}\\tG-Loss: {:.6f}'\n",
    "          .format(epoch, batch_idx * len(x), len(self.train_dataloader) * BATCH_SIZE, D_loss, G_loss))\n",
    "\n",
    "    train_D_loss /= len(self.train_dataloader) * BATCH_SIZE\n",
    "    train_G_loss /= len(self.train_dataloader) * BATCH_SIZE\n",
    "    if verbose:\n",
    "        print('====> Train | Epoch: {} \\t | \\tAverage D-loss: {:.4f} \\t | \\tAverage G-loss: {:.4f}'.format(epoch, train_D_loss, train_G_loss,))\n",
    "    return train_G_loss, train_D_loss\n",
    "  \n",
    "  def test(self, verbose=True):\n",
    "    \"\"\"\n",
    "    This function iterates over `test_dataloader` and\n",
    "    reports `generator` and `discriminator` losses.\n",
    "    \"\"\"\n",
    "    self.discriminator.eval()\n",
    "    self.generator.eval()\n",
    "\n",
    "    test_G_loss = 0\n",
    "    test_D_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for x, _ in self.test_dataloader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        #################### Problem 10 (1 points) ####################\n",
    "        # 1) Put `generator` in `eval` mode\n",
    "        # 2) Put `discriminator` in `train` mode\n",
    "        # 4) Compute `discriminator`'s loss; use `D_step`\n",
    "        ####################################################\n",
    "        pass\n",
    "        D_loss = pass\n",
    "        ####################### End ########################\n",
    "\n",
    "        #################### Problem 11 (1 points) ####################\n",
    "        # 1) Put `generator` in `train` mode\n",
    "        # 2) Put `discriminator` in `eval` mode\n",
    "        # 4) Compute `generator`'s loss; use `G_step(n)` where `n = x.shape[0]`\n",
    "        ####################################################\n",
    "        pass\n",
    "        G_loss = pass\n",
    "        ####################### End ########################\n",
    "\n",
    "        test_D_loss += D_loss.item() * x.shape[0]\n",
    "        test_G_loss += G_loss.item() * x.shape[0]\n",
    "\n",
    "            \n",
    "    test_D_loss /= len(self.test_dataloader) * BATCH_SIZE\n",
    "    test_G_loss /= len(self.test_dataloader) * BATCH_SIZE\n",
    "    if verbose:\n",
    "      print('====> Test | Average D-loss: {:.4f} \\t | \\tAverage G-loss: {:.4f}'.format(test_D_loss, test_G_loss,))\n",
    "    return test_G_loss, test_D_loss\n",
    "\n",
    "  def run(self, n_epoch, G_update_times=1, D_update_times=1, verbose=True):\n",
    "    \"\"\"\n",
    "    This function will optimize parameters of `generator` and `discriminator`\n",
    "    for `n_epoch` epochs on `train_dataloader` dataset and validate it on\n",
    "    `test_dataloader`. At the end of each epoch, `visualization_helper_fn`\n",
    "    will be called to visualize the GAN behavior.\n",
    "\n",
    "\n",
    "    Some arguments: \\n\n",
    "    `G_update_times`: how many times to update `generator` on each batch \\n\n",
    "    `D_update_times`: how many times to update `discriminator` on each batch \\n\n",
    "    \"\"\"\n",
    "    !gpustat\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        self.train_loop(epoch, G_update_times, D_update_times, verbose)\n",
    "        self.test(verbose)\n",
    "        self.visualization_helper_fn(self)\n",
    "        if epoch == 1:\n",
    "          !gpustat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iUrebfa-TUc"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4x8LJFGVn9JG"
   },
   "source": [
    "# 3) Part 1 - 2D Toy Dataset\n",
    "\n",
    "In the first part, we will learn density of toy datasets to examine GAN training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOofuIor51iJ"
   },
   "source": [
    "## 3.1) Helper functions\n",
    "These are functions used in visulization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o8rdZPi4YLt"
   },
   "outputs": [],
   "source": [
    "def toy2d_density_plot(samples, title, ax=None):\n",
    "  \"\"\"\n",
    "  This function plots the distribution of `samples`.\n",
    "  \"\"\"\n",
    "  fig = None\n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots(1, figsize=(3, 3))\n",
    "\n",
    "  samples = samples.detach().cpu().numpy()\n",
    "  sns.kdeplot(samples[:, 0], samples[:, 1], shade=True, ax=ax)\n",
    "  ax.set_title(title)\n",
    "\n",
    "  if fig is not None:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LzuQDSO335a"
   },
   "outputs": [],
   "source": [
    "def toy2d_decision_boundary_plot(samples, labels, title, ax=None):\n",
    "  \"\"\"\n",
    "  This function plots regions corresponding to each label value. Can be used to\n",
    "  understand the decision boundary of `discriminator`.\n",
    "  \"\"\"\n",
    "  fig = None\n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots(1, figsize=(3, 3))\n",
    "\n",
    "  samples = samples.detach().cpu().numpy()\n",
    "  labels = labels.detach().cpu().numpy()\n",
    "\n",
    "  sns.scatterplot(\n",
    "      x=samples[:, 0],\n",
    "      y=samples[:, 1],\n",
    "      hue=labels,\n",
    "      legend='brief',\n",
    "      marker='.',\n",
    "      ax=ax\n",
    "      )\n",
    "  ax.set_title(title)\n",
    "\n",
    "  if fig is not None:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpMDZTie51ia"
   },
   "outputs": [],
   "source": [
    "def toy2d_visulization_helper_fn(trainer: GANTrainer):\n",
    "  \"\"\"\n",
    "  Used to visualize generator's distribution and \n",
    "  discriminator's decision boundary.\n",
    "  \"\"\"\n",
    "  rows = 1\n",
    "  cols = 2\n",
    "  fig, axs = plt.subplots(rows, cols, squeeze=False, figsize=(3 * cols, 3 * rows))\n",
    "\n",
    "  _, fake_samples = trainer.generate_samples(10000)\n",
    "  random_samples = (torch.rand((10000, 2)) - .5) * 3 * R\n",
    "  random_samples = random_samples.to(device)\n",
    "  with torch.no_grad():\n",
    "    disc_labels = torch.sigmoid(trainer.discriminator(random_samples))[:, 0]\n",
    "  \n",
    "  toy2d_density_plot(fake_samples, 'generator distribution', axs[0][0])\n",
    "  toy2d_decision_boundary_plot(random_samples, disc_labels, 'discriminator decision boundary', axs[0][1])\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUIhdkjU51il"
   },
   "source": [
    "## 3.2) Defining Toy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mld1SJ1B51in"
   },
   "outputs": [],
   "source": [
    "class Toy2dCircularGaussiansDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"\n",
    "  This dataset puts 'n_modes` gaussians on a circle with 'r' radius.\n",
    "  \"\"\"\n",
    "  def __init__(self, r=10, std_dev=1, n_modes=20, n_samples=10000):\n",
    "    self.r = r\n",
    "    self.std_dev = std_dev\n",
    "    self.n_modes = n_modes\n",
    "    \n",
    "    self.mode_ids = torch.randint(high=self.n_modes, size=(n_samples,))\n",
    "    theta = 2 * np.pi / n_modes * self.mode_ids\n",
    "    mu = self.r * torch.stack([torch.cos(theta), torch.sin(theta)], dim=1)\n",
    "    self.samples = torch.randn((n_samples, 2)) * self.std_dev + mu\n",
    "  \n",
    "  def __getitem__(self, i):  \n",
    "    return self.samples[i], self.mode_ids[i]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxgMoU010Qft"
   },
   "outputs": [],
   "source": [
    "class Toy2dGridGaussiansDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"\n",
    "  This dataset puts 'n_modes^2` gaussians on a grid with '2r' width and height.\n",
    "  \"\"\"\n",
    "  def __init__(self, r=1, std_dev=1, n_modes=5, n_samples=10000):\n",
    "    self.r = r\n",
    "    self.std_dev = std_dev\n",
    "    self.n_modes = n_modes\n",
    "    assert n_modes >= 2\n",
    "    \n",
    "    self.mode_ids = torch.randint(high=self.n_modes, size=(n_samples, 2))\n",
    "    mu = self.mode_ids * (2 * self.r / (self.n_modes - 1)) - self.r\n",
    "    self.samples = torch.randn((n_samples, 2)) * self.std_dev + mu\n",
    "  \n",
    "  def __getitem__(self, i):  \n",
    "    return self.samples[i], self.mode_ids[i]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.samples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SxIgB7womPk"
   },
   "source": [
    "## 3.3) Dataset Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxrTviybrOYX"
   },
   "source": [
    "You can load either `Toy2dCircularGaussiansDataset` or `Toy2dGridGaussiansDataset` to play with it; but the question at the end of this part is for the `Toy2dGridGaussiansDataset` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWzCekNm0Odb"
   },
   "outputs": [],
   "source": [
    "# parameters of toy dataset\n",
    "TRAIN_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "R = 1\n",
    "\n",
    "# ############### To use `Toy2dCircularGaussiansDataset` ############### #\n",
    "# N_MODES = 8\n",
    "# STD_DEV = 25e-3\n",
    "# toy2d_train_dataset = Toy2dCircularGaussiansDataset(R, STD_DEV, N_MODES, TRAIN_SIZE)\n",
    "# toy2d_test_dataset = Toy2dCircularGaussiansDataset(R, STD_DEV, N_MODES, TEST_SIZE)\n",
    "# ###################################################################### #\n",
    "\n",
    "# ################# To use `Toy2dGridGaussiansDataset` ################# #\n",
    "N_MODES = 4\n",
    "STD_DEV = 1e-4\n",
    "toy2d_train_dataset = Toy2dGridGaussiansDataset(R, STD_DEV, N_MODES, TRAIN_SIZE)\n",
    "toy2d_test_dataset = Toy2dGridGaussiansDataset(R, STD_DEV, N_MODES, TEST_SIZE)\n",
    "# ###################################################################### #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4T13wZDkTg9"
   },
   "outputs": [],
   "source": [
    "# Ploting dataset distribution to see it :)\n",
    "\n",
    "toy2d_density_plot(toy2d_train_dataset[:10000][0], 'training data distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ksr5sGS51jI"
   },
   "source": [
    "## 3.4) Defining Models (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahAktS8j97iZ"
   },
   "outputs": [],
   "source": [
    "# Generator part\n",
    "#################### Problem 12 (2 points) ####################\n",
    "# 1) Define `toy2d_noise_generator`.\n",
    "# 2) Define `toy2d_generator` network.\n",
    "#    Use `Tanh` for intermediate layers.\n",
    "#    Use `Linear` for the last layer.\n",
    "####################################################\n",
    "toy2d_noise_generator = NoiseGenerator(pass)\n",
    "\n",
    "toy2d_generator = nn.Sequential(\n",
    "    pass\n",
    ")\n",
    "####################### End ########################\n",
    "\n",
    "# Discriminator part\n",
    "#################### Problem 13 (2 points) ####################\n",
    "# 1) Define `toy2d_discriminator` network.\n",
    "#    Use `Tanh` for intermediate layers.\n",
    "#    Use `Linear` for the last layer. why?\n",
    "####################################################\n",
    "toy2d_discriminator = nn.Sequential(\n",
    "    pass\n",
    ")\n",
    "####################### End ########################\n",
    "\n",
    "toy2d_generator = toy2d_generator.to(device)\n",
    "toy2d_discriminator = toy2d_discriminator.to(device)\n",
    "\n",
    "print(toy2d_generator)\n",
    "print(toy2d_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gvy_rvpH51jW"
   },
   "source": [
    "## 3.5) Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mEMbouf9fJZ"
   },
   "source": [
    "### 3.5.1) Batching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsiQM-ri51i0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "toy2d_dataloader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE, \n",
    "    'shuffle': True,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "toy2d_train_dataloader = torch.utils.data.DataLoader(toy2d_train_dataset, **toy2d_dataloader_kwargs)\n",
    "toy2d_test_dataloader = torch.utils.data.DataLoader(toy2d_test_dataset, **toy2d_dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1APhQx8w9lI-"
   },
   "source": [
    "### 3.5.2) Run (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53dLDZBQ51jW"
   },
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 50\n",
    "#################### Problem 14 (1 points) ####################\n",
    "# Tune `generator` and `discriminator` learning rates\n",
    "# and also the `G_LOSS_MODE`.\n",
    "####################################################\n",
    "TOY2D_G_LEARNING_RATE = pass\n",
    "TOY2D_D_LEARNING_RATE = pass\n",
    "G_LOSS_MODE = pass\n",
    "####################### End ########################\n",
    "\n",
    "toy2d_trainer = GANTrainer(toy2d_train_dataloader, toy2d_test_dataloader,\n",
    "                           toy2d_noise_generator, toy2d_generator, toy2d_discriminator,\n",
    "                           TOY2D_G_LEARNING_RATE, TOY2D_D_LEARNING_RATE,\n",
    "                           toy2d_visulization_helper_fn, G_LOSS_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79eXE2cYvgg5"
   },
   "outputs": [],
   "source": [
    "#################### Problem 15 (1 points) ####################\n",
    "# Tune `n_epoch`, `G_update_times` and `D_update_times`\n",
    "####################################################\n",
    "toy2d_trainer.run(n_epoch=pass, G_update_times=pass, D_update_times=pass, verbose=True)\n",
    "####################### End ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAyg-dWzMrr7"
   },
   "source": [
    "### 3.5.3) Final Visualization\n",
    "It is an evaluating criteria for network architecture and parameter tuning (problems 12 to 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY8kqIe8MgGz"
   },
   "outputs": [],
   "source": [
    "toy2d_visulization_helper_fn(toy2d_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfsPZGg7n0SR"
   },
   "source": [
    "# 4) Part 2 - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUz8p5t1z8m0"
   },
   "source": [
    "In the second part, we will learn to generate MNIST like samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6u5VGTbrS4E"
   },
   "source": [
    "## 4.1) Helper functions\n",
    "These are functions used in visulization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GGaLtw0Py0f"
   },
   "outputs": [],
   "source": [
    "def vector_linspace(start, end, steps):\n",
    "  \"\"\"\n",
    "  Vector version of torch linspace\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for dim in range(start.shape[0]):\n",
    "    result.append(torch.linspace(start[dim], end[dim], steps))\n",
    "  result = torch.stack(result, dim=1).to(device)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_K3w1YkLHiKF"
   },
   "outputs": [],
   "source": [
    "def mnist_show(image_batch, rows=1):\n",
    "  \"\"\"\n",
    "  This function gets multiple MNIST images and plots them in the given number of rows.\n",
    "  \"\"\"\n",
    "  image_batch = image_batch.detach().cpu()\n",
    "  image_batch = image_batch.view(-1, 28, 28)\n",
    "  image_batch = image_batch.numpy()\n",
    "\n",
    "  cols = np.ceil(image_batch.shape[0] / rows)\n",
    "  plt.rcParams['figure.figsize'] = (cols, rows) # set default size of plots  TODO float(folan)?\n",
    "  \n",
    "  for i in range(image_batch.shape[0]):\n",
    "      plt.subplot(rows, cols, i + 1)\n",
    "      plt.imshow(image_batch[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "      plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8heA_hapEbQ"
   },
   "outputs": [],
   "source": [
    "def mnist_visulization_helper_fn(trainer: GANTrainer):\n",
    "  \"\"\"\n",
    "  Generates fake samples and plots them for you.\n",
    "  \"\"\"\n",
    "  _, fake_samples = trainer.generate_samples(30)\n",
    "  mnist_show(fake_samples, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9qGDX6uu05D"
   },
   "outputs": [],
   "source": [
    "def show_interpolations(trainer:GANTrainer, n_rows, n_cols):\n",
    "    \"\"\"\n",
    "    Shows image interpolation (grid of [`n_rows`, `n_cols`]) in input noise space.\n",
    "    \"\"\"\n",
    "    anchor_noises = trainer.noise_generator(4)\n",
    "    left_column = vector_linspace(anchor_noises[0], anchor_noises[1], n_rows)\n",
    "    right_column = vector_linspace(anchor_noises[2], anchor_noises[3], n_rows)\n",
    "    rows = []\n",
    "    for i in range(n_rows):\n",
    "      rows.append(vector_linspace(left_column[i], right_column[i], n_cols))\n",
    "    noises = torch.stack(rows, dim=0).view(n_rows * n_cols, -1)\n",
    "    with torch.no_grad():\n",
    "      fake_imgs = trainer.generator(noises)\n",
    "    mnist_show(fake_imgs, n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PrjBkfKVNvp"
   },
   "source": [
    "## 4.2) Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHVgo551guUb"
   },
   "outputs": [],
   "source": [
    "# Get MNIST dataloader\n",
    "\n",
    "mnist_dataset_kwargs = {\n",
    "    'download': True,\n",
    "    'transform': transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     lambda img: img * 2 - 1, # to make pixel values between [-1, 1]\n",
    "                                    #  lambda img: torch.transpose(img, 1, 2)\n",
    "                                     ])\n",
    "}\n",
    "\n",
    "mnist_train_dataset = datasets.MNIST('./data', train=True, **mnist_dataset_kwargs)\n",
    "mnist_test_dataset = datasets.MNIST('./data', train=False, **mnist_dataset_kwargs)\n",
    "\n",
    "print(len(mnist_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvx7jyzrq8KT"
   },
   "outputs": [],
   "source": [
    "temp = torch.stack([mnist_train_dataset[i][0] for i in range(30)], dim=0)\n",
    "mnist_show(temp, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFH2dkj-q64d"
   },
   "source": [
    "## 4.3) Defining Models (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HX1-1FLyFWIC"
   },
   "outputs": [],
   "source": [
    "# Generator part\n",
    "#################### Problem 16 (3 points) ####################\n",
    "# 1) Define `mnist_noise_generator`.\n",
    "# 2) Define `mnist_generator` network.\n",
    "#    Use `LeakyReLU` for intermediate layers.\n",
    "#    Use `Tanh` for the last layer.\n",
    "#    Use convolution layers.\n",
    "#    Can use batch norm, dropout, ...\n",
    "####################################################\n",
    "mnist_noise_generator = NoiseGenerator(pass)\n",
    "\n",
    "mnist_generator = nn.Sequential(\n",
    "    pass\n",
    ")\n",
    "####################### End ########################\n",
    "\n",
    "# Discriminator part\n",
    "#################### Problem 17 (3 points) ####################\n",
    "# 1) Define `toy2d_discriminator` network.\n",
    "#    Use `LeakyReLU` for intermediate layers.\n",
    "#    Use `Linear` for the last layer. why?\n",
    "#    Use convolution layers.\n",
    "#    Can use batch norm, dropout, ...\n",
    "####################################################\n",
    "mnist_discriminator = nn.Sequential(\n",
    "    pass\n",
    ")\n",
    "####################### End ########################\n",
    "\n",
    "mnist_generator = mnist_generator.to(device)\n",
    "mnist_discriminator = mnist_discriminator.to(device)\n",
    "\n",
    "print(mnist_generator)\n",
    "print(mnist_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRowa40urDqt"
   },
   "source": [
    "## 4.4) Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRzIAHXM9qqW"
   },
   "source": [
    "### 4.4.1) Batching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JSIu1s10QhX"
   },
   "source": [
    "you can use `train_sampler` and `test_sampler` to test your model on smaller subset of dataset; but, the final report must be on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gs1-BL0d8zRp"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "mnist_dataloader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE, \n",
    "    'pin_memory': True,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "# ############### To use complete dataset ############### #\n",
    "train_sampler = None\n",
    "test_sampler = None\n",
    "mnist_dataloader_kwargs['shuffle'] = True\n",
    "# ####################################################### #\n",
    "\n",
    "# ############### To use subsample of dataset ############### #\n",
    "# train_sampler = torch.utils.data.RandomSampler(mnist_train_dataset, replacement=True, num_samples=10000)\n",
    "# test_sampler = torch.utils.data.RandomSampler(mnist_test_dataset, replacement=True, num_samples=2000)\n",
    "# mnist_dataloader_kwargs['shuffle'] = False\n",
    "# ########################################################### #\n",
    "\n",
    "mnist_train_dataloader = torch.utils.data.DataLoader(mnist_train_dataset, sampler=train_sampler, **mnist_dataloader_kwargs)\n",
    "mnist_test_dataloader = torch.utils.data.DataLoader(mnist_test_dataset, sampler=test_sampler, **mnist_dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdHxGQmE9uU2"
   },
   "source": [
    "### 4.4.2) Run (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rs_QWICkoplR"
   },
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 50\n",
    "#################### Problem 18 (1 points) ####################\n",
    "# Tune `generator` and `discriminator` learning rates\n",
    "# and also the `G_LOSS_MODE`.\n",
    "####################################################\n",
    "MNIST_G_LEARNING_RATE = pass\n",
    "MNIST_D_LEARNING_RATE = pass\n",
    "G_LOSS_MODE = pass\n",
    "####################### End ########################\n",
    "\n",
    "mnist_trainer = GANTrainer(mnist_train_dataloader, mnist_test_dataloader,\n",
    "                           mnist_noise_generator, mnist_generator, mnist_discriminator,\n",
    "                           MNIST_G_LEARNING_RATE, MNIST_D_LEARNING_RATE,\n",
    "                           mnist_visulization_helper_fn, G_LOSS_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4uoX2noTUxw"
   },
   "outputs": [],
   "source": [
    "#################### Problem 19 (1 points) ####################\n",
    "# Tune `n_epoch`, `G_update_times` and `D_update_times`\n",
    "####################################################\n",
    "mnist_trainer.run(n_epoch=pass, G_update_times=pass, D_update_times=pass, verbose=True)\n",
    "####################### End ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUHevpnQuwq3"
   },
   "source": [
    "### 4.4.3) Final Visualization\n",
    "It is an evaluating criteria for network architecture and parameter tuning (problems 16 to 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVgOTRq25_KP"
   },
   "outputs": [],
   "source": [
    "mnist_visulization_helper_fn(mnist_trainer)\n",
    "show_interpolations(mnist_trainer, 10, 10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
